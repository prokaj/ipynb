{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Logistic Regression is a Python adaptation of p. 161-163 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Adapted by R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,\\\n",
    "    QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix,\\\n",
    "    classification_report, precision_score, ConfusionMatrixDisplay\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.3 Linear Discriminant Analysis\n",
    "Let's return to the `Smarket` data from `ISLR`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.19130</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.29650</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.41120</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.27600</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.20570</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.88850</td>\n",
       "      <td>0.043</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>1.28581</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>2005</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>1.54047</td>\n",
       "      <td>0.130</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>2005</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.42236</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>2005</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>0.130</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.38254</td>\n",
       "      <td>-0.489</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
       "0     2001  0.381 -0.192 -2.624 -1.055  5.010  1.19130  0.959        Up\n",
       "1     2001  0.959  0.381 -0.192 -2.624 -1.055  1.29650  1.032        Up\n",
       "2     2001  1.032  0.959  0.381 -0.192 -2.624  1.41120 -0.623      Down\n",
       "3     2001 -0.623  1.032  0.959  0.381 -0.192  1.27600  0.614        Up\n",
       "4     2001  0.614 -0.623  1.032  0.959  0.381  1.20570  0.213        Up\n",
       "...    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
       "1245  2005  0.422  0.252 -0.024 -0.584 -0.285  1.88850  0.043        Up\n",
       "1246  2005  0.043  0.422  0.252 -0.024 -0.584  1.28581 -0.955      Down\n",
       "1247  2005 -0.955  0.043  0.422  0.252 -0.024  1.54047  0.130        Up\n",
       "1248  2005  0.130 -0.955  0.043  0.422  0.252  1.42236 -0.298      Down\n",
       "1249  2005 -0.298  0.130 -0.955  0.043  0.422  1.38254 -0.489      Down\n",
       "\n",
       "[1250 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ISLRdata(name):\n",
    "    import statsmodels.datasets as smd\n",
    "    df = smd.get_rdataset(name, 'ISLR')\n",
    "    df.data.__doc__ = df.__doc__\n",
    "    df.data.__name__ = df.title\n",
    "    return df.data\n",
    "   \n",
    "df = ISLRdata('Smarket')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://github.com/prokaj/ipynb/week1'\n",
    "\n",
    "# df = pd.read_csv('Smarket.csv', \n",
    "#                  usecols=range(1,10), \n",
    "#                  index_col=0, \n",
    "#                  parse_dates=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform LDA on the `Smarket` data from the `ISLR` package. In `Python`, we can fit a LDA model using the `LinearDiscriminantAnalysis()` function, which is part of the `discriminant_analysis` module of the `sklearn` library. As we did with logistic regression and KNN, we'll fit the model using only the observations before 2005, and then test the model on the data from 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49198397 0.50801603]\n"
     ]
    }
   ],
   "source": [
    "idx = df.Year<2005\n",
    "X_train = df[idx][['Lag1','Lag2']]\n",
    "y_train = df[idx]['Direction']\n",
    "\n",
    "X_test = df[~idx][['Lag1','Lag2']]\n",
    "y_test = df[~idx]['Direction']\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "model = lda.fit(X_train, y_train)\n",
    "\n",
    "print(model.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA output indicates prior probabilities of ${\\hat{\\pi}}_1 = 0.492$ and ${\\hat{\\pi}}_2 = 0.508$; in other words,\n",
    "49.2% of the training observations correspond to days during which the\n",
    "market went down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "print(model.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above provides the group means; these are the average\n",
    "of each predictor within each class, and are used by LDA as estimates\n",
    "of $\\mu_k$. These suggest that there is a tendency for the previous 2 days’\n",
    "returns to be negative on days when the market increases, and a tendency\n",
    "for the previous days’ returns to be positive on days when the market\n",
    "declines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05544078 -0.0443452 ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of linear discriminants output provides the linear\n",
    "combination of `Lag1` and `Lag2` that are used to form the LDA decision rule.\n",
    "\n",
    "If $−0.0554\\times{\\tt Lag1}−0.0443\\times{\\tt Lag2}$ is large, then the LDA classifier will\n",
    "predict a market increase, and if it is small, then the LDA classifier will\n",
    "predict a market decline. **Note**: these coefficients differ from those produced by `R`.\n",
    "    \n",
    "The `predict()` function returns a list of LDA’s predictions about the movement of the market on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Down': 70, 'Up': 182}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(dict(zip(*np.unique(pred, return_counts=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assigned 70 observations to the \"Down\" class, and 182 observations to the \"Up\" class. Let's check out the confusion matrix to see how this model is doing. We'll want to compare the **predicted class** (which we can find in `pred`) to the **true class** (found in `y\\_test})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.500     0.315     0.387       111\n",
      "          Up      0.582     0.752     0.656       141\n",
      "\n",
      "    accuracy                          0.560       252\n",
      "   macro avg      0.541     0.534     0.522       252\n",
      "weighted avg      0.546     0.560     0.538       252\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(confusion_matrix(pred, y_test, labels=lda.classes_))\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "ConfusionMatrixDisplay.from_estimator(lda, X_test, y_test)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.75      0.75      0.75         4\n",
      "   macro avg       0.75      0.83      0.73         4\n",
      "weighted avg       0.88      0.75      0.77         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 2'] #, 'class 2']\n",
    "print(classification_report(y_true, y_pred, \n",
    "                            labels=[0,2],\n",
    "                            target_names=target_names))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the F1 score, also known as balanced F-score or F-measure.\n",
       "\n",
       "The F1 score can be interpreted as a weighted average of the precision and\n",
       "recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
       "The relative contribution of precision and recall to the F1 score are\n",
       "equal. The formula for the F1 score is::\n",
       "\n",
       "    F1 = 2 * (precision * recall) / (precision + recall)\n",
       "\n",
       "In the multi-class and multi-label case, this is the average of\n",
       "the F1 score of each class with weighting depending on the ``average``\n",
       "parameter.\n",
       "\n",
       "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : array-like, default=None\n",
       "    The set of labels to include when ``average != 'binary'``, and their\n",
       "    order if ``average is None``. Labels present in the data can be\n",
       "    excluded, for example to calculate a multiclass average ignoring a\n",
       "    majority negative class, while labels not present in the data will\n",
       "    result in 0 components in a macro average. For multilabel targets,\n",
       "    labels are column indices. By default, all labels in ``y_true`` and\n",
       "    ``y_pred`` are used in sorted order.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Parameter `labels` improved for multiclass problem.\n",
       "\n",
       "pos_label : str or int, default=1\n",
       "    The class to report if ``average='binary'`` and the data is binary.\n",
       "    If the data are multiclass or multilabel, this will be ignored;\n",
       "    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
       "    scores for that label only.\n",
       "\n",
       "average : {'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'\n",
       "    This parameter is required for multiclass/multilabel targets.\n",
       "    If ``None``, the scores for each class are returned. Otherwise, this\n",
       "    determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'binary'``:\n",
       "        Only report results for the class specified by ``pos_label``.\n",
       "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by counting the total true positives,\n",
       "        false negatives and false positives.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average weighted\n",
       "        by support (the number of true instances for each label). This\n",
       "        alters 'macro' to account for label imbalance; it can result in an\n",
       "        F-score that is not between precision and recall.\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average (only\n",
       "        meaningful for multilabel classification where this differs from\n",
       "        :func:`accuracy_score`).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division, i.e. when all\n",
       "    predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
       "    but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "f1_score : float or array of float, shape = [n_unique_labels]\n",
       "    F1 score of the positive class in binary classification or weighted\n",
       "    average of the F1 scores of each class for the multiclass task.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
       "multilabel_confusion_matrix\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `Wikipedia entry for the F1-score\n",
       "       <https://en.wikipedia.org/wiki/F1_score>`_.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import f1_score\n",
       ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
       ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
       ">>> f1_score(y_true, y_pred, average='macro')\n",
       "0.26...\n",
       ">>> f1_score(y_true, y_pred, average='micro')\n",
       "0.33...\n",
       ">>> f1_score(y_true, y_pred, average='weighted')\n",
       "0.26...\n",
       ">>> f1_score(y_true, y_pred, average=None)\n",
       "array([0.8, 0. , 0. ])\n",
       ">>> y_true = [0, 0, 0, 0, 0, 0]\n",
       ">>> y_pred = [0, 0, 0, 0, 0, 0]\n",
       ">>> f1_score(y_true, y_pred, zero_division=1)\n",
       "1.0...\n",
       ">>> # multilabel classification\n",
       ">>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
       ">>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
       ">>> f1_score(y_true, y_pred, average=None)\n",
       "array([0.66666667, 1.        , 0.66666667])\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When ``true positive + false positive == 0``, precision is undefined.\n",
       "When ``true positive + false negative == 0``, recall is undefined.\n",
       "In such cases, by default the metric will be set to 0, as will f-score,\n",
       "and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
       "modified with ``zero_division``.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "metrics.f1_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.4 Quadratic Discriminant Analysis\n",
    "We will now fit a QDA model to the `Smarket` data. QDA is implemented\n",
    "in `sklearn` using the `QuadraticDiscriminantAnalysis()` function, which is again part of the `discriminant_analysis` module. The\n",
    "syntax is identical to that of `LinearDiscriminantAnalysis()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "model2 = qda.fit(X_train, y_train)\n",
    "print(model2.priors_)\n",
    "print(model2.means_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains the group means. But it does not contain the coefficients\n",
    "of the linear discriminants, because the QDA classifier involves a\n",
    "_quadratic_, rather than a linear, function of the predictors. The `predict()`\n",
    "function works in exactly the same fashion as for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pred2=model2.predict(X_test)\n",
    "print(np.unique(pred2, return_counts=True))\n",
    "print(confusion_matrix(pred2, y_test))\n",
    "print(classification_report(y_test, pred2, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the QDA predictions are accurate almost 60% of the time,\n",
    "even though the 2005 data was not used to fit the model. This level of accuracy\n",
    "is quite impressive for stock market data, which is known to be quite\n",
    "hard to model accurately. \n",
    "\n",
    "This suggests that the quadratic form assumed\n",
    "by QDA may capture the true relationship more accurately than the linear\n",
    "forms assumed by LDA and logistic regression. However, we recommend\n",
    "evaluating this method’s performance on a larger test set before betting\n",
    "that this approach will consistently beat the market!\n",
    "\n",
    "# An Application to Carseats Data\n",
    "Let's see how the `LDA/QDA` approach performs on the `Carseats` data set, which is\n",
    "included with `ISLR`. \n",
    "\n",
    "Recall: this is a simulated data set containing sales of child car seats at 400 different stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Carseats.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can build a model that predicts `ShelveLoc`, the shelf location (Bad, Good, or Medium) of the product at each store. Don't forget to hold out some of the data for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get credit for this lab, please post your answers to the prompt in [#lab5](https://sds293.slack.com/messages/C7CR96LJ3)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
