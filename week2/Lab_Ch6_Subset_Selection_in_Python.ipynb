{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Subset Selection is a Python adaptation of p. 244-247 of \"Introduction to Statistical Learning with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It is based on the work of R. Jordan Crouser at Smith College for SDS293: Machine Learning (Spring 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.datasets as smd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5.1 Best Subset Selection\n",
    "\n",
    "Here we apply the best subset selection approach to the Hitters data. We\n",
    "wish to predict a baseball playerâ€™s Salary on the basis of various statistics\n",
    "associated with performance in the previous year. Let's take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "use_smith_edu = False  or True\n",
    "if use_smith_edu:\n",
    "    url = ''\n",
    "    hitters_df = pd.read_csv('https://www.science.smith.edu/~jcrouser/SDS293/data/Hitters.csv')\n",
    "    hitters_df = hitters_df.set_index('Player')\n",
    "else:\n",
    "    hitters_df = smd.get_rdataset('Hitters', 'ISLR')\n",
    "    hitters_df.data.__doc__ = hitters_df.__doc__\n",
    "    hitters_df = hitters_df.data\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the `Salary` variable is missing for some of the\n",
    "players. The `isnull()` function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a `TRUE` value\n",
    "for any elements that are missing, and a `FALSE` value for non-missing elements.\n",
    "The `sum()` function can then be used to count all of the missing elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `Salary` is missing for 59 players. The `dropna()` function\n",
    "removes all of the rows that have missing values in any variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df_clean = hitters_df.dropna()\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df_clean.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df_clean[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(hitters_df_clean[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = hitters_df_clean.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "hitters = hitters_df_clean.drop(['League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "hitters = pd.concat([hitters, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform best subset selection by identifying the best model that contains a given number of predictors, where **best** is quantified using RSS. We'll define a helper function to outputs the best set of variables for\n",
    "each model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itertools.combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def haty_norm2(y, X):\n",
    "    q, r = np.linalg.qr(X)\n",
    "    return (np.dot(q.T, y)**2).sum()\n",
    "\n",
    "def best_k(y, X, k):\n",
    "    best_v = 0\n",
    "    best_c = None\n",
    "    for c in itertools.combinations(range(X.shape[1]),k):\n",
    "        v = haty_norm2(y, X[:,c])\n",
    "        if v > best_v:\n",
    "            best_v = v\n",
    "            best_c = c\n",
    "    return best_c, best_v\n",
    "\n",
    "def best_subset(y, X, max_k=8):\n",
    "    col_names = X.columns\n",
    "    y, X = np.asarray(y), np.asarray(X)\n",
    "    y = y - y.mean(axis=0)\n",
    "    X = X - X.mean(axis=0)\n",
    "    q,r = np.linalg.qr(X)\n",
    "    TSS = (y**2).sum()\n",
    "    y = np.dot(q.T, y)\n",
    "    def result(x):\n",
    "        c,v = x\n",
    "        return {\n",
    "            'num_pred': len(c),\n",
    "            'vars': [col_names[i] for i in c],\n",
    "            'R2': v/TSS\n",
    "        }\n",
    "    return pd.DataFrame([result(best_k(y, r, k+1)) for k in tqdm(range(max_k))]).set_index('num_pred')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit = pd.DataFrame(best_subset(hitters.Salary, hitters.drop(columns=['Salary']), 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit['model'] = [sm.OLS.from_formula(f, data=hitters).fit()\n",
    "              for f in fit.vars.apply(lambda x: 'Salary ~ ' + ' + '.join(sorted(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(y, X, best_v, best_subset, k=0, v_offset=0):\n",
    "    idx = np.arange(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        x, X0 = X[:,i], X[:,idx!=i]\n",
    "        traverse_tree(y, X0, best_v, best_subset, k=k, v_offset=v_offset)\n",
    "        x = x/np.linalg.norm(x) \n",
    "        X0 = X0 - np.dot(x,X0)\n",
    "        c = np.dot(x, y)\n",
    "        y0 = y - c*x\n",
    "        traverse_tree(y0, X0, best_v, best_subset, k=k+1, v_offset=v_offset+c**2)\n",
    "        \n",
    "def bestsubset(y, X):\n",
    "    y, X = np.asarray(y), np.asarray(X)\n",
    "    y = y - y.mean(axis=0)\n",
    "    X = X - X.mean(axis=0)\n",
    "    q, r = np.linalg.qr(X)\n",
    "    TSS = (y**2).sum()\n",
    "    y, X = np.dot(q.T, y), r\n",
    "    p, = y.shape\n",
    "    best_v = np.zeros(p)\n",
    "    best_subset = [None]*p\n",
    "    traverse_tree()\n",
    "    return best_v/TSS, best_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def processSubset(feature_set):\n",
    "#     # Fit model on feature_set and calculate RSS\n",
    "#     model = sm.OLS(y,X[list(feature_set)])\n",
    "#     regr = model.fit()\n",
    "#     RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "#     return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def getBest(k):\n",
    "    \n",
    "#     tic = time.time()\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for combo in itertools.combinations(X.columns, k):\n",
    "#         results.append(processSubset(combo))\n",
    "    \n",
    "#     # Wrap everything up in a nice dataframe\n",
    "#     models = pd.DataFrame(results)\n",
    "    \n",
    "#     # Choose the model with the highest RSS\n",
    "#     best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "#     toc = time.time()\n",
    "#     print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "#     # Return the best model, along with some other useful information about the model\n",
    "#     return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `DataFrame` containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # Could take quite a while to complete...\n",
    "\n",
    "# models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "# tic = time.time()\n",
    "# for i in range(1,4):\n",
    "#     models_best.loc[i] = getBest(i)\n",
    "\n",
    "# toc = time.time()\n",
    "# print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big `DataFrame` that contains the best models we've generated along with their RSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# models_best['model'][2].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the details of each model, no problem! We can get a full rundown of a single model using the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(fit.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best two-variable model\n",
    "contains only `Hits` and `CRBI`. To save time, we only generated results\n",
    "up to the best 7-variable model. You can use the functions we defined above to explore as many variables as are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Show the best 19-variable model (there's actually only one)\n",
    "print(fit.model.iloc()[-1].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than letting the results of our call to the `summary()` function print to the screen, we can access just the parts we need using the model's attributes. For example, if we want the $R^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fit.loc[2, \"model\"].rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! In addition to the verbose output we get when we print the summary to the screen, fitting the `OLM` also produced many other useful statistics such as adjusted $R^2$, AIC, and BIC. We can examine these to try to select the best overall model. Let's start by looking at $R^2$ across all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "print('R2:', fit.model.apply(lambda x: x.rsquared), sep='\\n')\n",
    "print('RSS:', fit.model.apply(lambda x: x.ssr), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $R^2$ statistic increases monotonically as more\n",
    "variables are included.\n",
    "\n",
    "Plotting RSS, adjusted $R^2$, AIC, and BIC for all of the models at once will\n",
    "help us decide which model to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 16, 'lines.markersize': 8})\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt_data = np.stack(fit.apply(lambda x: (len(x['vars']), x['model'].ssr), axis=1))\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(*plt_data.T)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "plt.grid()\n",
    "plt.plot(*plt_data[plt_data[:,1].argmin()], 'or')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "plt_data = np.stack(fit.apply(lambda x: (len(x['vars']), x['model'].rsquared_adj), axis=1))\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(*plt_data.T)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "plt.grid()\n",
    "plt.plot(*plt_data[plt_data[:,1].argmax()], 'or')\n",
    "\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "plt_data = np.stack(fit.apply(lambda x: (len(x['vars']), x['model'].aic), axis=1))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(*plt_data.T)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "plt.grid()\n",
    "plt.plot(*plt_data[plt_data[:,1].argmin()], 'or')\n",
    "\n",
    "\n",
    "plt_data = np.stack(fit.apply(lambda x: (len(x['vars']), x['model'].bic), axis=1))\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(*plt_data.T)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')\n",
    "plt.grid()\n",
    "plt.plot(*plt_data[plt_data[:,1].argmin()], 'or')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the second step of our selection process, we narrowed the field down to just one model on any $k<=p$ predictors. We see that according to BIC, the best performer is the model with 6 variables. According to AIC and adjusted $R^2$ something a bit more complex might be better. Again, no one measure is going to give us an entirely accurate picture... but they all agree that a model with 5 or fewer predictors is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5.2 Forward and Backward Stepwise Selection\n",
    "We can also use a similar approach to perform forward stepwise\n",
    "or backward stepwise selection, using a slight modification of the functions we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(y, X):\n",
    "    col_names = list(X.columns)\n",
    "    y, X = np.asarray(y), np.asarray(X)\n",
    "    y = y - y.mean(axis=0)\n",
    "    X = X - X.mean(axis=0)\n",
    "    q, X = np.linalg.qr(X)\n",
    "    TSS = (y**2).sum()\n",
    "    y = np.dot(q.T, y)\n",
    "    tv = 0\n",
    "    result = []\n",
    "    predictors = []\n",
    "    while len(col_names)>0:\n",
    "        (c,), v = best_k(y, X, 1)\n",
    "        tv += v\n",
    "        predictors.append(col_names.pop(c))\n",
    "        result.append(dict(num_pred=len(predictors), predictors=predictors.copy(), r2=tv/TSS))\n",
    "        x, X = X[:,c], X[:,np.arange(X.shape[1])!=c] \n",
    "        x = x/np.linalg.norm(x)\n",
    "        y = y - np.dot(y,x)*x\n",
    "        X = X - np.outer(x, np.dot(x.T, X))\n",
    "    return pd.DataFrame(result).set_index('num_pred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how much faster it runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_fit = forward(hitters.Salary, hitters.drop(columns=['Salary']))\n",
    "fwd_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_on(v):\n",
    "    return sm.OLS.from_formula('Salary ~ ' + ' + '.join(sorted(v)), data=hitters).fit()\n",
    "\n",
    "fwd_fit['model'] = [regress_on(v) for v in fwd_fit.predictors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! That's a lot better. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_fit.apply(lambda x: (x['model'].rsquared, x['r2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(fwd_fit.loc[1, \"model\"].summary())\n",
    "print(fwd_fit.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using forward stepwise selection, the best one-variable\n",
    "model contains only `Hits`, and the best two-variable model additionally\n",
    "includes `CRBI`. Let's see how the models stack up against best subset selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(fit.loc[7, \"model\"].params)\n",
    "print(fwd_fit.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, the best one-variable through six-variable\n",
    "models are each identical for best subset and forward selection.\n",
    "\n",
    "# Backward Selection\n",
    "Not much has to change to implement backward selection... just looping through the predictors in reverse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y, X):\n",
    "    col_names = list(X.columns)\n",
    "    y, X = np.asarray(y), np.asarray(X)\n",
    "    y = y - y.mean(axis=0)\n",
    "    X = X - X.mean(axis=0)\n",
    "    q, X = np.linalg.qr(X)\n",
    "    TSS = (y**2).sum()\n",
    "    y = np.dot(q.T, y)\n",
    "    tv = 0\n",
    "    result = []\n",
    "    while len(col_names)>1:\n",
    "        c, v = best_k(y, X, len(col_names)-1)\n",
    "        col_names = [col_names[i] for i in c]\n",
    "        result.append(dict(num_pred=len(c), \n",
    "                           predictors=col_names, \n",
    "                           r2=v/TSS))\n",
    "        X = X[:,c] \n",
    "    return pd.DataFrame(result).set_index('num_pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_fit = backward(hitters.Salary, hitters.drop(columns=['Salary']))\n",
    "print(bwd_fit)\n",
    "bwd_fit['model'] = [regress_on(v) for v in bwd_fit.predictors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, the best one-variable through six-variable\n",
    "models are each identical for best subset and forward selection.\n",
    "However, the best seven-variable models identified by forward stepwise selection,\n",
    "backward stepwise selection, and best subset selection are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"------------\")\n",
    "print(\"Best Subset:\")\n",
    "print(\"------------\")\n",
    "print(fit.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(fwd_fit.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(bwd_fit.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
